import arxiv
import csv
from datetime import datetime, timedelta
import time
import os
import requests
from urllib.parse import urlencode
import re
from bs4 import BeautifulSoup


def load_signal_authors(file_path):
    """åŠ è½½ä¿¡å·æºçœ‹æ¿ä¸­çš„ä½œè€…åå•"""
    authors = set()
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('name'):
                    author_name = row['name'].strip().title()
                    authors.add(author_name)
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ä¿¡å·æºæ–‡ä»¶å‡ºé”™: {e}")
    return authors

def get_daily_submission_count(category, type):
    """è·å–æŒ‡å®šç±»åˆ«çš„æ–°æäº¤è®ºæ–‡æ•°é‡"""
    try:
        url = f"https://arxiv.org/list/{category}/{type}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        if response.status_code == 429:
            retry = int(response.headers.get("Retry-After", 10))
            print(f"â³ è¢«é™æµï¼Œç­‰å¾… {retry} ç§’åé‡è¯•...")
            time.sleep(retry)
            response = requests.get(url, headers=headers, timeout=10)

        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾"Total of XX entries"æ–‡æœ¬
        total_text = soup.find(string=re.compile(r'Total of \d+ entries'))
        if total_text:
            # æå–æ•°å­—
            match = re.search(r'Total of (\d+) entries', total_text)
            if match:
                count = int(match.group(1))
                print(f"   ğŸ“Š {category}: {count} ç¯‡æ–°æäº¤")
                return count
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
        dd_tags = soup.find_all('dd')
        if dd_tags:
            count = len(dd_tags)
            print(f"   ğŸ“Š {category}: ~{count} ç¯‡æ–°æäº¤ (å¤‡ç”¨è®¡æ•°)")
            return count
            
        print(f"   âš ï¸ {category}: æ— æ³•è·å–æäº¤æ•°é‡")
        return 0
        
    except Exception as e:
        print(f"   âŒ {category}: è·å–æäº¤æ•°é‡å¤±è´¥ - {e}")
        return 0

def get_category_submission_counts(categories, type):
    """æ‰¹é‡è·å–æ‰€æœ‰ç±»åˆ«çš„å½“å¤©æäº¤æ•°é‡"""
    print("ğŸ” è·å–å„ç±»åˆ«å½“å¤©æ–°æäº¤æ•°é‡...")
    category_counts = {}
    total_count = 0
    
    for category in categories:
        count = get_daily_submission_count(category, type)
        category_counts[category] = count
        total_count += count
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    
    print(f"ğŸ“ˆ æ€»è®¡: {total_count} ç¯‡æ–°æäº¤è®ºæ–‡")
    return category_counts, total_count

def normalize_author_name(name):
    """æ ‡å‡†åŒ–ä½œè€…åå­—æ ¼å¼"""
    return name.strip().title()

def clean_abstract(abstract):
    """æ¸…ç†å’Œæ ¼å¼åŒ–æ‘˜è¦æ–‡æœ¬"""
    if not abstract:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
    cleaned = abstract.strip().replace('\n', ' ').replace('\r', ' ')
    # æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    cleaned = ' '.join(cleaned.split())
    
    return cleaned

def format_arxiv_url(entry_id):
    """æ ¼å¼åŒ–arXivé“¾æ¥ä¸ºæ ‡å‡†æ ¼å¼"""
    # ä»entry_idä¸­æå–è®ºæ–‡ID
    if '/' in entry_id:
        paper_id = entry_id.split('/')[-1]
    else:
        paper_id = entry_id
    
    # ç§»é™¤ç‰ˆæœ¬å·ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if 'v' in paper_id and paper_id.split('v')[-1].isdigit():
        paper_id = paper_id.split('v')[0]
    
    # ç¡®ä¿è¿”å›æ ‡å‡†çš„arXiv abs URLæ ¼å¼
    return f"https://arxiv.org/abs/{paper_id}"


def _ping_arxiv():
    """åªæµ‹è¿é€šæ€§ï¼Œä¸æµ‹æ•°æ®é‡"""
    try:
        h = {"User-Agent": "QijiFetcher/1.0 (contact: youremail@example.com)"}
        r = requests.get(
            "https://export.arxiv.org/api/query",
            params={"search_query": "all:ai", "max_results": 1},
            headers=h, timeout=20
        )
        return r.status_code == 200
    except Exception:
        return False

def test_arxiv_connection():
    """æµ‹è¯• arXivï¼šä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢æœ€æ–°è®ºæ–‡"""
    try:
        print("ğŸ” æµ‹è¯•arXivè¿æ¥...")
        os.environ["ARXIV_USER_AGENT"] = "QijiFetcher/1.0 (contact: youremail@example.com)"
        if not _ping_arxiv():
            print("âŒ è¿é€šæ€§æµ‹è¯•å¤±è´¥ï¼ˆæ— æ³•è·å¾— 200ï¼‰")
            return False

        import arxiv
        client = arxiv.Client(page_size=5, delay_seconds=5, num_retries=8)
        
        # ä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢æœ€è¿‘3å¤©çš„è®ºæ–‡
        today = datetime.now().date()
        start_date = today - timedelta(days=3)
        date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')}0000 TO {today.strftime('%Y%m%d')}2359]"
        
        search = arxiv.Search(
            query=f"cat:cs.AI AND {date_query}",
            max_results=5,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        results = list(client.results(search))
        if results:
            print("âœ… arXivè¿æ¥æ­£å¸¸ï¼ˆæ‹¿åˆ°æœ€æ–°ç»“æœï¼‰")
            latest_paper = results[0]
            print(f"   æœ€æ–°è®ºæ–‡ç¤ºä¾‹: {latest_paper.title[:50]}...")
            print(f"   å‘å¸ƒæ—¥æœŸ: {latest_paper.published.date()}")
            print(f"   æ›´æ–°æ—¥æœŸ: {latest_paper.updated.date()}")
        else:
            print("âš ï¸ è¿æ¥æ­£å¸¸ï¼Œä½†æœ€è¿‘3å¤©æ²¡æœ‰ cs.AI è®ºæ–‡")
        return True
    except Exception as e:
        print(f"âŒ arXivè¿æ¥æµ‹è¯•å¤±è´¥ï¼ˆå¼‚å¸¸ï¼‰ï¼š{e}")
        return False


def probe_api_batch_top_dates(start_date, end_date):
    """æ‰¹æ¬¡æ¢é’ˆï¼šä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢ç¡®è®¤èƒ½å¦è·å–åˆ°æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡"""
    try:
        client = arxiv.Client(page_size=10, delay_seconds=3.2)
        
        # æ„é€ æ—¥æœŸèŒƒå›´æŸ¥è¯¢
        date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')}0000 TO {end_date.strftime('%Y%m%d')}2359]"
        
        search = arxiv.Search(
            query=f"cat:cs.AI AND {date_query}",
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
            max_results=5
        )
        top = []
        for r in client.results(search):
            top.append((r.published.date().isoformat(), r.title[:60]))
        
        if top:
            print("ğŸ§ª API é¡¶éƒ¨5æ¡è®ºæ–‡æ—¥æœŸï¼š")
            for d, t in top:
                print("   ", d, "|", t)
            return [d for d, _ in top]
        else:
            print("âš ï¸ æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
            return []
            
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡æ¢é’ˆå¤±è´¥: {e}")
        return []

def should_proceed_today(latest_dates_iso, expect_floor):
    """
    æ£€æŸ¥APIæ‰¹æ¬¡æ˜¯å¦å·²æ›´æ–°åˆ°æœŸæœ›çš„æ—¥æœŸ
    latest_dates_iso: probe_api_batch_top_dates() è¿”å›çš„æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨
    expect_floor: ä½ å¸Œæœ›è‡³å°‘çœ‹åˆ°çš„æœ€æ™šæ—¥æœŸï¼ˆæ¯”å¦‚ä»Šå¤©-1å¤©ï¼Œæˆ– start_dateï¼‰
    """
    if not latest_dates_iso:
        print("âš ï¸ æ— æ³•è·å–APIé¡¶éƒ¨æ—¥æœŸï¼Œä½†ä»ç»§ç»­æ‰§è¡Œ")
        return True
    
    try:
        # API å·²æ»šåˆ° >= æœŸæœ›ä¸‹é™æ—¥æœŸï¼Œæ‰ç»§ç»­æŠ“
        latest = max(datetime.fromisoformat(d).date() for d in latest_dates_iso)
        if latest >= expect_floor:
            print(f"âœ… APIå·²æœ‰ {latest} çš„è®ºæ–‡ï¼Œæ»¡è¶³æœŸæœ›æ—¥æœŸ {expect_floor}")
            return True
        else:
            print(f"â¸ï¸ APIæœ€æ–°è®ºæ–‡æ—¥æœŸæ˜¯ {latest}ï¼Œå°šæœªåˆ° {expect_floor}ã€‚å…ˆè·³è¿‡æœ¬æ¬¡æŠ“å–ã€‚")
            return False
    except Exception as e:
        print(f"âš ï¸ è§£ææ—¥æœŸæ—¶å‡ºé”™: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
        return True

def get_latest_papers(signal_authors, start_date, end_date, category_counts=None):
    """è·å–æœ€æ–°è®ºæ–‡çš„æ ¸å¿ƒå‡½æ•° - ä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢"""
    
    print(f"ğŸ“… æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {end_date}")
    print(f"ğŸ“Š ä¿¡å·æºä½œè€…æ•°é‡ï¼š{len(signal_authors)}")
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    output_file = f"latest_arxiv_papers_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
    
    counter = 1
    base_name, ext = os.path.splitext(output_file)
    while os.path.exists(output_file):
        output_file = f"{base_name}_{counter}{ext}"
        counter += 1
    
    # æ›´ä¿å®ˆçš„å®¢æˆ·ç«¯é…ç½®
    client = arxiv.Client(
        page_size=100,  # å¢åŠ é¡µé¢å¤§å°æé«˜æ•ˆç‡
        delay_seconds=3,
        num_retries=5
    )
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_papers = 0
    matched_papers = 0
    date_stats = {}
    category_stats = {}
    processed_papers = set()
    all_papers_data = []
    
    print(f"\nğŸ” å¼€å§‹æŸ¥è¯¢è®ºæ–‡ï¼ˆä½¿ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤ï¼‰...")
    
    # æ„é€ æ—¥æœŸèŒƒå›´æŸ¥è¯¢å­—ç¬¦ä¸²
    date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')}0000 TO {end_date.strftime('%Y%m%d')}2359]"
    print(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: {date_query}")
    
    # è®¡ç®—æœºç§‘å­¦ç›¸å…³ç±»åˆ«ï¼ˆåŒ…æ‹¬äº¤å‰å­¦ç§‘ï¼‰
    categories = [
        # ä¸»è¦AI/MLç›¸å…³
        "cs.AI", "cs.LG", "cs.CV", "cs.CL", "cs.NE", "cs.RO", "cs.IR",
        # ç³»ç»Ÿå’Œæ¶æ„
        "cs.DC", "cs.OS", "cs.AR", "cs.PF", "cs.NI",
        # è½¯ä»¶å·¥ç¨‹å’Œç¼–ç¨‹è¯­è¨€
        "cs.SE", "cs.PL", "cs.LO", "cs.FL",
        # ç†è®ºè®¡ç®—æœºç§‘å­¦
        "cs.DS", "cs.CC", "cs.DM", "cs.GT", "cs.IT",
        # åº”ç”¨é¢†åŸŸ
        "cs.DB", "cs.CR", "cs.GR", "cs.MM", "cs.HC", "cs.CY", "cs.ET",
        # æ•°å­¦è®¡ç®—
        "cs.NA", "cs.MS", "cs.SC", "cs.CE",
        # å…¶ä»–
        "cs.OH", "cs.SY",
        # äº¤å‰å­¦ç§‘
        "stat.ML", "math.OC", "math.ST", "eess.IV", "eess.SP", "eess.AS",
        "econ.EM", "q-bio.QM", "physics.data-an"
    ]
    
    for category in categories:
        print(f"\nğŸ” æŸ¥è¯¢ç±»åˆ«: {category}")
        
        # è·å–è¯¥ç±»åˆ«çš„é¢„æœŸè®ºæ–‡æ•°é‡
        expected_count = category_counts.get(category, 0) if category_counts else 0
        
        try:
            # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ—¥æœŸèŒƒå›´è¿‡æ»¤
            query = f"cat:{category} AND {date_query}"
            
            # æ ¹æ®é¢„æœŸæ•°é‡è°ƒæ•´æŸ¥è¯¢ä¸Šé™
            max_results = max(expected_count * 2, 200) if expected_count > 0 else 2000
            print(f"ğŸ“Š é¢„æœŸè®ºæ–‡æ•°: {expected_count}, æŸ¥è¯¢ä¸Šé™: {max_results}")
            print(f"ğŸ” æŸ¥è¯¢è¯­å¥: {query}")
            
            search = arxiv.Search(
                query=query,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending,
                max_results=max_results
            )
            
            category_count = 0
            category_matched = 0
            processed_in_category = 0
            
            print(f"      ğŸ”„ å¼€å§‹è·å–è®ºæ–‡...")
            
            for result in client.results(search):
                try:
                    pub_date = result.published.date()
                    update_date = result.updated.date()
                    paper_id = result.entry_id.split('/')[-1]
                    processed_in_category += 1
                    
                    # æ¯å¤„ç†50ç¯‡æ˜¾ç¤ºè¿›åº¦
                    if processed_in_category % 50 == 0:
                        print(f"      ğŸ“Š å·²å¤„ç† {processed_in_category} ç¯‡")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ç¯‡è®ºæ–‡
                    if paper_id in processed_papers:
                        continue
                    
                    # æ—¥æœŸèŒƒå›´å·²ç»åœ¨æŸ¥è¯¢ä¸­è¿‡æ»¤äº†ï¼Œè¿™é‡Œç›´æ¥å¤„ç†
                    authors = [normalize_author_name(a.name) for a in result.authors]
                    
                    # å°†è®ºæ–‡IDæ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
                    processed_papers.add(paper_id)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    date_str = pub_date.strftime("%Y-%m-%d")
                    date_stats[date_str] = date_stats.get(date_str, 0) + 1
                    category_stats[result.primary_category] = category_stats.get(result.primary_category, 0) + 1
                    
                    # æ£€æŸ¥åŒ¹é…çš„ä½œè€…
                    matched_authors = [a for a in authors if a in signal_authors]
                    
                    # åªæœ‰åŒ¹é…åˆ°ä¿¡å·æºä½œè€…çš„è®ºæ–‡æ‰å†™å…¥æ•°æ®
                    if matched_authors:
                        formatted_url = format_arxiv_url(result.entry_id)
                        
                        # å‡†å¤‡è®ºæ–‡æ•°æ®
                        paper_data = {
                            'id': paper_id,
                            'title': result.title.strip().replace('\n', ' '),
                            'authors': ", ".join(authors),
                            'matched_authors': ", ".join(matched_authors),
                            'published_date': pub_date,
                            'primary_category': result.primary_category,
                            'all_categories': ", ".join(result.categories),
                            'abstract': clean_abstract(result.summary),
                            'abs_url': formatted_url,
                            'has_match': True
                        }
                        
                        all_papers_data.append(paper_data)
                        category_matched += 1
                        matched_papers += 1
                        print(f"         âœ… [{pub_date}] åŒ¹é…: {', '.join(matched_authors)} | {result.title[:40]}...")
                    
                    category_count += 1
                    total_papers += 1
                    
                    # å¦‚æœå·²è·å–è¶³å¤Ÿæ•°é‡ä¸”æœ‰é¢„æœŸï¼Œå¯ä»¥æå‰ç»“æŸ
                    if expected_count > 0 and category_count >= expected_count * 1.5:
                        print(f"      âœ… å·²è·å–è¶³å¤Ÿè®ºæ–‡ï¼Œæå‰ç»“æŸ")
                        break
                        
                except Exception as e:
                    print(f"         âš ï¸ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"      ğŸ“Š {category}: {category_count}ç¯‡è®ºæ–‡, {category_matched}ç¯‡åŒ¹é…")
            
        except Exception as e:
            print(f"      âŒ ç±»åˆ« {category} æŸ¥è¯¢å‡ºé”™: {e}")
            continue
    
    # å†™å…¥CSVæ–‡ä»¶ - åªå†™å…¥åŒ¹é…çš„è®ºæ–‡
    print(f"\nğŸ“ å†™å…¥CSVæ–‡ä»¶ï¼Œå…± {len(all_papers_data)} ç¯‡åŒ¹é…è®ºæ–‡...")
    
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["ID", "Title", "Authors", "Matched_Authors", "Published_Date","Primary_Category", "All_Categories", "Abstract", "Abstract_URL"])
        
        # æŒ‰å‘å¸ƒæ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        all_papers_data.sort(key=lambda x: x['published_date'], reverse=True)
        
        for paper in all_papers_data:
            writer.writerow([
                paper['id'],
                paper['title'],
                paper['authors'],
                paper['matched_authors'],
                paper['published_date'],
                paper['primary_category'],
                paper['all_categories'],
                paper['abstract'],
                paper['abs_url']
            ])
    
    print(f"âœ… æ•°æ®å†™å…¥å®Œæˆï¼Œå…±å†™å…¥ {len(all_papers_data)} ç¯‡åŒ¹é…è®ºæ–‡")
    
    return output_file, total_papers, matched_papers, date_stats, category_stats

def preview_csv(file_path):
    """é¢„è§ˆCSVæ–‡ä»¶å‰å‡ è¡Œ"""
    print(f"\nğŸ“„ {file_path} é¢„è§ˆ:")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for i, line in enumerate(lines[:4]):
                print(f"   {line.strip()}")
    except Exception as e:
        print(f"âš ï¸ é¢„è§ˆæ–‡ä»¶å‡ºé”™: {e}")

def print_results(output_file, total_papers, matched_papers, date_stats, category_stats):
    """æ‰“å°ç»“æœç»Ÿè®¡"""
    print(f"\nğŸ“Š çˆ¬å–å®Œæˆ!")
    print(f"   â€¢ æ€»è®ºæ–‡æ•°: {total_papers}")
    print(f"   â€¢ åŒ¹é…è®ºæ–‡æ•°: {matched_papers}")
    print(f"   â€¢ åŒ¹é…ç‡: {matched_papers/total_papers*100:.1f}%" if total_papers > 0 else "   â€¢ åŒ¹é…ç‡: 0%")
    
    if date_stats:
        print(f"\nğŸ“… æŒ‰å‘å¸ƒæ—¥æœŸåˆ†å¸ƒ:")
        for date in sorted(date_stats.keys(), reverse=True):
            weekday = datetime.strptime(date, "%Y-%m-%d").strftime("%A")
            print(f"   {date} ({weekday}): {date_stats[date]}ç¯‡")
    
    if category_stats:
        print(f"\nğŸ“‚ æŒ‰ç±»åˆ«åˆ†å¸ƒ (å‰10):")
        sorted_categories = sorted(category_stats.items(), key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:
            print(f"   {category}: {count}ç¯‡")
    
    print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    if matched_papers > 0:
        print(f"ğŸ‰ æˆåŠŸåŒ¹é…åˆ° {matched_papers} ç¯‡ä¿¡å·æºä½œè€…è®ºæ–‡!")
        print("ğŸ“‹ CSVæ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—ï¼š")
        print("   - ID: è®ºæ–‡ID")
        print("   - Title: è®ºæ–‡æ ‡é¢˜")
        print("   - Authors: æ‰€æœ‰ä½œè€…")
        print("   - Matched_Authors: åŒ¹é…çš„ä¿¡å·æºä½œè€…")
        print("   - Published_Date: é¦–æ¬¡å‘å¸ƒæ—¥æœŸ")
        print("   - Updated_Date: æœ€åæ›´æ–°æ—¥æœŸ")
        print("   - Primary_Category: ä¸»è¦ç±»åˆ«")
        print("   - All_Categories: æ‰€æœ‰ç±»åˆ«")
        print("   - Abstract: è®ºæ–‡æ‘˜è¦")
        print("   - Abstract_URL: è®ºæ–‡æ‘˜è¦é“¾æ¥")
        preview_csv(output_file)
    else:
        print("âš ï¸ æ²¡æœ‰åŒ¹é…åˆ°ä¿¡å·æºä½œè€…è®ºæ–‡")
        print("   å¯èƒ½çš„åŸå› :")
        print("   1. è¯¥æ—¶é—´æ®µç¡®å®æ²¡æœ‰ä¿¡å·æºä½œè€…å‘è¡¨è®ºæ–‡")
        print("   2. ä½œè€…åå­—æ ¼å¼ä¸åŒ¹é…")
        print("   3. éœ€è¦æ£€æŸ¥ä¿¡å·æºä½œè€…åå•")
        if total_papers > 0:
            print(f"   (å…±æ‰«æäº† {total_papers} ç¯‡è®ºæ–‡ï¼Œä½†éƒ½ä¸åŒ¹é…)")
        else:
            print("   (æœªè·å–åˆ°ä»»ä½•è®ºæ–‡æ•°æ®)")
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…è®ºæ–‡ï¼Œåˆ é™¤ç©ºçš„CSVæ–‡ä»¶
        if os.path.exists(output_file):
            os.remove(output_file)
            print(f"   å·²åˆ é™¤ç©ºçš„è¾“å‡ºæ–‡ä»¶: {output_file}")

def get_latest_papers_by_days(signal_file, days_back=1):
    """æ ¹æ®æŒ‡å®šå¤©æ•°è·å–æœ€æ–°è®ºæ–‡çš„ä¸»å‡½æ•°"""
    
    print("ğŸš€ è·å–æœ€æ–°è®ºæ–‡æ¨¡å¼")
    print("=" * 30)
    
    # åŠ è½½ä¿¡å·æºä½œè€…
    signal_authors = load_signal_authors(signal_file)
    if not signal_authors:
        print("âš ï¸ æœªåŠ è½½åˆ°æœ‰æ•ˆçš„ä¿¡å·æºä½œè€…åå•")
        return None
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    today = datetime.now().date()
    start_date = today - timedelta(days=days_back)

    print(f"ğŸ“… æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼š{start_date} è‡³ {today}")
    print(f"ğŸ“Š æŸ¥è¯¢å¤©æ•°ï¼š{days_back} å¤©")
    
    # æ‰¹æ¬¡æ¢é’ˆï¼šæ£€æŸ¥APIæ˜¯å¦æœ‰æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡
    print("\nğŸ§ª æ‰§è¡Œæ‰¹æ¬¡æ¢é’ˆæ£€æŸ¥...")
    top_dates = probe_api_batch_top_dates(start_date, today)
    if not should_proceed_today(top_dates, start_date):
        print("ğŸ’¤ æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æš‚æ— è®ºæ–‡ï¼Œè·³è¿‡æœ¬æ¬¡æŠ“å–")
        return None
    
    category_counts = None
    categories = [
        "cs.AI", "cs.LG", "cs.CV", "cs.CL", "cs.NE", "cs.RO", "cs.IR",
        "cs.DC", "cs.OS", "cs.AR", "cs.PF", "cs.NI",
        "cs.SE", "cs.PL", "cs.LO", "cs.FL",
        "cs.DS", "cs.CC", "cs.DM", "cs.GT", "cs.IT",
        "cs.DB", "cs.CR", "cs.GR", "cs.MM", "cs.HC", "cs.CY", "cs.ET",
        "cs.NA", "cs.MS", "cs.SC", "cs.CE",
        "cs.OH", "cs.SY",
        # äº¤å‰å­¦ç§‘
        "stat.ML", "math.OC", "math.ST", "eess.IV", "eess.SP", "eess.AS",
        "econ.EM", "q-bio.QM", "physics.data-an"
    ]
    
    # å¦‚æœæŸ¥è¯¢æœ€è¿‘1å¤©ï¼Œå…ˆè·å–å½“å¤©å„ç±»åˆ«çš„æäº¤æ•°é‡
    if days_back == 1:
        type = "new"
        category_counts, total_expected = get_category_submission_counts(categories, type)
        print(f"ğŸ“Š é¢„æœŸæ€»è®ºæ–‡æ•°: {total_expected}")        
        time.sleep(3.2)

    elif days_back > 1:
        type = "recent"
        category_counts, total_expected = get_category_submission_counts(categories, type)
        print(f"ğŸ“Š é¢„æœŸæ€»è®ºæ–‡æ•°: {total_expected}")
        time.sleep(3.2)
        
    return get_latest_papers(signal_authors, start_date, today, category_counts)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ arXivæœ€æ–°è®ºæ–‡è·å–å·¥å…· (ä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢)")
    print("=" * 50)
    
    # æ£€æŸ¥ä¿¡å·æºæ–‡ä»¶
    signal_file = "ä¿¡å·æºä¿¡æ¯.csv"

    if not os.path.exists(signal_file):
        print(f"âŒ æ‰¾ä¸åˆ°ä¿¡å·æºæ–‡ä»¶: {signal_file}")
        print("è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        print("ä¿¡å·æºæ–‡ä»¶åº”åŒ…å« 'name' åˆ—ï¼Œå­˜å‚¨ä½œè€…å§“å")
        return
    
    print(f"ğŸ“‹ ä½¿ç”¨ä¿¡å·æºæ–‡ä»¶: {signal_file}")
    
    ok = test_arxiv_connection()
    if not ok:
        print("âŒ æ— æ³•è¿æ¥åˆ° arXivï¼ˆè¿é€šæ€§å¤±è´¥ï¼‰")
        return
    
    # è·å–ç”¨æˆ·è¾“å…¥
    days_input = input("è¯·è¾“å…¥è¦æŸ¥è¯¢çš„å¤©æ•° (é»˜è®¤2å¤©): ").strip()
    try:
        days_back = int(days_input) if days_input else 2
        if days_back <= 0:
            days_back = 2
    except ValueError:
        days_back = 2
    
    print(f"ğŸ“… å°†æŸ¥è¯¢æœ€è¿‘ {days_back} å¤©çš„è®ºæ–‡")
    
    # æ‰§è¡Œè·å–æœ€æ–°è®ºæ–‡
    result = get_latest_papers_by_days(signal_file, days_back)
    if result:
        print_results(*result)
    else:
        print("âŒ è·å–è®ºæ–‡å¤±è´¥")

if __name__ == "__main__":
    main()